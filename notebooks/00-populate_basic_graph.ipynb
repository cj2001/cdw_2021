{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Let's get started with Method 1...\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install py2neo\n",
    "!pip install wikipedia\n",
    "!pip install spacy==3.0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configure spacy\n",
    "\n",
    "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
    "\n",
    "In general, to load the models we use the following command:\n",
    "\n",
    "`python3 -m spacy download en_core_web_md`\n",
    "\n",
    "This has already been completed in the container, but if you want to add other models you can do this either as a cell in this notebook or via the CLI.\n",
    "\n",
    "## API key for Google Knowledge Graph\n",
    "\n",
    "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory.  If you are doing this on Google Colab, mount your local Google Drive directory and store it there."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## If you are running this in Google Colab, install the language model using this command:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python -m spacy download en_core_web_md"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "api_key = open('.api_key').read()\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "print(non_nc.pipe_names)\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query Google Knowledge Graph\n",
    "\n",
    "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions for text cleaning\n",
    "\n",
    "As with any data science project, we have a lot of cleaning to do!  These next functions do things like removing \n",
    "\n",
    "- special characters (`remove_special_characters`)\n",
    "- stop words and punctuation (`remove_stop_words_and_punct`)\n",
    "- dates (`remove_dates`)\n",
    "- duplicates (`remove_duplicates`)\n",
    "  - Duplicates crop up many times during this process and we will be battling them a lot!\n",
    "  \n",
    "Then, we can get to the heart of the matter, `create_svo_triples`."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text, print_lists=False):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's get started!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "text= wikipedia.summary('barack obama')\n",
    "text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Barack Hussein Obama II ( (listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American  president of the United States. He previously served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\nObama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate. In 2008, he was nominated by the Democratic Party for president a year after beginning his campaign, and after a close primary campaign against Hillary Clinton. Obama was elected over Republican nominee John McCain in the general election and was inaugurated alongside his running mate, Joe Biden, on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate.\\nObama signed many landmark bills into law during his first two years in office. The main reforms that were passed include the Affordable Care Act (commonly referred to as ACA or \"Obamacare\"), although without a public health insurance option, the Dodd–Frank Wall Street Reform and Consumer Protection Act, and the Don\\'t Ask, Don\\'t Tell Repeal Act of 2010. The American Recovery and Reinvestment Act of 2009 and Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010 served as economic stimuli amidst the Great Recession. After a lengthy debate over the national debt limit, he signed the Budget Control and the American Taxpayer Relief Acts. In foreign policy, he increased U.S. troop levels in Afghanistan, reduced nuclear weapons with the United States–Russia New START treaty, and ended military involvement in the Iraq War. He ordered military involvement in Libya for the implementation of the UN Security Council Resolution 1973, contributing to the overthrow of Muammar Gaddafi. He also ordered the military operation that resulted in the killing of Osama bin Laden.\\nAfter winning re-election by defeating Republican opponent Mitt Romney, Obama was sworn in for a second term in 2013. During this term, he promoted inclusion for LGBT Americans. His administration filed briefs that urged the Supreme Court to strike down same-sex marriage bans as unconstitutional (United States v. Windsor and Obergefell v. Hodges); same-sex marriage was legalized nationwide in 2015 after the Court ruled so in Obergefell. He advocated for gun control in response to the Sandy Hook Elementary School shooting, indicating support for a ban on assault weapons, and issued wide-ranging executive actions concerning global warming and immigration. In foreign policy, he ordered successful military interventions in Iraq and Syria in response to gains made by ISIL after the 2011 withdrawal from Iraq, continued the process of ending U.S. combat operations in Afghanistan in 2016, promoted discussions that led to the 2015 Paris Agreement on global climate change, initiated sanctions against Russia following the invasion in Ukraine and again after interference in the 2016 U.S. elections, brokered the JCPOA nuclear deal with Iran, and normalized U.S. relations with Cuba. Obama nominated three justices to the Supreme Court: Sonia Sotomayor and Elena Kagan were confirmed as justices, while Merrick Garland faced partisan obstruction from the Republican-led Senate led by Mitch McConnell, which never held hearings or a vote on the nomination. Obama left office in January 2017 and continues to reside in Washington, D.C.During Obama\\'s terms in office, the United States\\' reputation abroad, as well as the American economy, significantly improved. Obama\\'s presidency has generally been regarded favorably, and evaluations of his presidency among historians, political scientists, and the general public frequently place him among the upper tier of American presidents.'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More helper functions\n",
    "\n",
    "Now that we have our text clean, we are ready to start getting information about all of the objects in the list (`get_obj_properties`) such as:\n",
    "\n",
    "- any identified node labels\n",
    "- any descriptions\n",
    "- any URLs\n",
    "\n",
    "These will be used to create node properties for all of the objects.  We end this section with allowing the ML models build into spacy to create word vector embeddings for each node based on the node description.  (If no description is provided, this will be an array of zeros.)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        try:\n",
    "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
    "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
    "        except:\n",
    "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[3]:\n",
    "            doc = nlp(tup[3])\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
    "        else:\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's now run this step by step..."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%time\n",
    "initial_tup_ls = create_svo_triples(text, print_lists=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 50.8 s, sys: 19.8 ms, total: 50.9 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "initial_tup_ls[0:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('oh bah mə', 'be', 'american politician'),\n",
       " ('oh bah mə', 'be', '44th president'),\n",
       " ('oh bah mə', 'be', 'united states')]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%time\n",
    "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "#clean_edge_ls = check_for_string_labels(edge_ls)\n",
    "#clean_edge_ls[0:3]\n",
    "clean_edge_ls = edge_ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 21.1 s, sys: 309 ms, total: 21.4 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "edge_ls[0:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('oh bah mə', 'be', 'american politician', '', ['Thing'], ''),\n",
       " ('oh bah mə', 'be', '44th president', [], [], []),\n",
       " ('oh bah mə',\n",
       "  'be',\n",
       "  'united states',\n",
       "  'The United States of America, commonly known as the United States or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions. ',\n",
       "  ['Thing', 'Country', 'Place', 'AdministrativeArea'],\n",
       "  'https://en.wikipedia.org/wiki/United_States')]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%%time\n",
    "edges_word_vec_ls = create_word_vectors(edge_ls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 4.3 s, sys: 3.99 ms, total: 4.31 s\n",
      "Wall time: 4.31 s\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the node and edge lists to populate the graph with the below helper functions\n",
    "\n",
    "These functions achieve the following:\n",
    "\n",
    "1. Deduping of the node list (`dedup`)\n",
    "2. Under the idea that we might want to use word embeddings, we are pulling them in as a node property.  However, Neo4j doesn't work well with numpy arrays, so we convert that array to a list of floats (`convert_vec_to_ls`).\n",
    "3. Add nodes to the graph with the `py2neo` bulk importer (`add_nodes`)\n",
    "4. Add edges (relationships) to the graph (`add_edges`)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[4]]\n",
    "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup\n",
    "\n",
    "\n",
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating some lists of tuples representing the node and edge lists\n",
    "\n",
    "For our node list, we begin with the query subject (Barack Obama), which is in `edge_ls[0][0]` and put this in the variable `orig_node_tup_ls`.  We assume a node label of `Subject` and no description or word vector.  We then add all of the objects associated with the `edges_word_vec_ls` (`obj_node_tup_ls`) and combine that with the previous variable to create `full_node_tup_ls`, which we then dedupe into `dedup_node_tup_ls`."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "dedup_node_tup_ls = dedup(full_node_tup_ls)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "len(full_node_tup_ls), len(dedup_node_tup_ls)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(681, 539)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the node list that will be used to populate the graph..."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Populate the graph\n",
    "\n",
    "Here we establish a connection to the database, which is running on the internal Docker network called `neo4j`.  We provide it the user name and password and also create a class for matching nodes (used when we establish the edges in the graph).  Finally, we add the nodes and edges to populate the database."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
    "# If you are using a Docker container for your DB, use the uncommented line.\n",
    "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
    "\n",
    "graph = Graph(\"bolt://neo4j:7687\", name=\"neo4j\", password=\"kgDemo\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "add_nodes(node_tup_ls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nodes in graph:  539\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "add_edges(edges_word_vec_ls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 70/70 [00:08<00:00,  8.39it/s]\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entity disambiguation\n",
    "\n",
    "In this notebook we will calculate the cosine similarity of the word vectors of target nodes to try and determine the likelihood of two nodes being the same entity.  Note that you could do this directly with spacy (which defaults to cosine similarity) prior to anything above through something like:\n",
    "\n",
    "```\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc1.similarity(doc2)\n",
    "```\n",
    "\n",
    "However, since we have already calculated the word vectors for each node description above, we will just use the cosine similarity built into `scikit-learn`. \n",
    "\n",
    "Note that we are creating now a complete node list since the nodes we are comparing might be either in the Barack or the Michelle Obama node lists..."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def get_word_vec_similarity(node1, node2, node_ls):\n",
    "    \n",
    "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
    "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
    "    \n",
    "    return cosine_similarity(node1_vec, node2_vec)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'president barack obama', dedup_node_tup_ls)\n",
    "print(cs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.9999997]]\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ...OK, that one was cheating...\n",
    "\n",
    "...because the Google Knowledge Graph knew that these were the same thing.  The NER returned these as being different nodes, but Google knew better and gave the exact description for each.  This will not necessarily be the case in more complicated knowledge graphs."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'mitch mcconnell', dedup_node_tup_ls)\n",
    "print(cs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.93807256]]\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem!\n",
    "\n",
    "There is a key figure missing from this graph!  (Hint: in Cypher, try the following command:\n",
    "\n",
    "```\n",
    "MATCH (n:Node)\n",
    "WHERE n.name CONTAINS 'michelle'\n",
    "RETURN n\n",
    "```\n",
    "\n",
    "In this next notebook, we are going to rectify this and build out the graph a little further."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}